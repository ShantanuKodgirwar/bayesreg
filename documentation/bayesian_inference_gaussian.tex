\documentclass[11pt]{article}
\usepackage[immediate]{silence}
\WarningFilter[temp]{latex}{Command} % silence the warning
\usepackage{sectsty}
\DeactivateWarningFilters[temp] % So nothing unrelated gets silenced
\usepackage{geometry}
\geometry{a4paper}

\usepackage{graphicx} 
\usepackage{subfigure}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{amsmath}
\usepackage{booktabs}
%\usepackage{fullpage}
\usepackage{mathrsfs, bm, bbm, natbib}

\renewcommand{\baselinestretch}{1.3}

\usepackage{geometry}
\geometry{a4paper}

\usepackage{url} 
\usepackage{helvet}
\usepackage[stable]{footmisc}
\usepackage{enumerate}
\usepackage{url}
\usepackage{mathrsfs}
\usepackage{xcolor}

\renewcommand\familydefault{phv}
\usepackage[helvet]{sfmath}
%
\sectionfont{\bfseries\upshape\large}
\subsectionfont{\bfseries\upshape\normalsize}
\subsubsectionfont{\bfseries\upshape\normalsize} 

\newcommand{\one}{\mathbbm{1}}
\newcommand{\eye}{\bm{I}}
\newcommand{\mean}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\mle}[1]{{#1}_{\text{\tiny ML}}}
\newcommand{\map}[1]{{#1}_{\text{\tiny MAP}}}
\newcommand\given[1][]{\:#1\vert\:}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\title{Bayesian Inference}
\date{\today}
\author{Shantanu Kodgirwar}

\begin{document}

\maketitle

\section{Maximum likelihood estimation}

Input vectors are given as $\bm{x} = (x_1, \ldots, x_N)^T$ and the output/target variables as $\bm{t} = (t_1, \ldots, t_N)^T$ and the polynomial coefficients as $\bm{w} = (w_1, \ldots, w_M)^T$.
\begin{eqnarray}
        t_{i}
        &=& \sum_{i=1}^N y(x_i, \bm{w})\\
        &=&
        \sum_{i=1}^N \sum_{k=1}^M y_k(x_i)w_k\\
        &=&
        (\bm{X}\bm{w})_i + n_i
\end{eqnarray}

Assumed distribution of $n_i$:
\begin{equation}\label{eqn:model}
n_i \sim \mathcal{N}(0, \sigma^2)
\end{equation}

The values of $t$, given the values of $x$ follows a Gaussian distribution.
\begin{equation}
    t_i \sim \mathcal{N}(y(x_i, \bm{w}), \sigma^2)
\end{equation}

The likelihood function is defined as follows.
\begin{equation}
    p(t_i \given \bm{X}, \bm{w}, \sigma^2) = \sqrt{\frac{1}{2\pi\sigma^2}}\exp{\Big[-\frac{1}{2\sigma^2} \left(t_i - (\bm{X}\bm{w})_i\right)^2\Big]}
\end{equation}

A precision parameter $\beta$ is defined which is given as $\beta^{-1} = \sigma^2$. 
Thus, we have the following modified likelihood function as follows.
\begin{equation}
    p(t_i \given \bm{X}, \bm{w}, \beta) = \sqrt{\frac{\beta}{2\pi}}\exp{\Big[-\frac{\beta}{2} \left(t_i - (\bm{X}\bm{w})_i\right)^2\Big]}
\end{equation}

Assuming the data is drawn independently, the likelihood is the joint probability given as the product of individual marginal probabilities. It is also assumed the value of $\beta$ is known or assumed. 
\begin{equation}\label{eqn:likelihood}
    p(\bm{t} \given \bm{X}, \bm{w}, \beta) = \prod_{i=1}^N p(t_i \given \bm{X}, \bm{w}, \beta)
\end{equation}

The log likelihood of equation \ref{eqn:likelihood} is given as 
\begin{eqnarray} \label{eqn:log_likelihood}
    \ln p(\bm{t} \given \bm{X}, \bm{w}, \beta) 
    &=&
    \sum_{i=1}^N \ln p(t_i \given \bm{X}, \bm{w}, \beta) \\
    &=&
    \sum_{i=1}^N \ln{\Big\{\sqrt{\frac{\beta}{2\pi}}\exp{\Big[\frac{-\beta}{2} \left(t_i - (\bm{X}\bm{w})_i\right)^2\Big]}\Big\}} \\
    &=&
    \frac{N}{2}\ln{\beta} - \frac{N}{2}\ln{2\pi} - \frac{\beta}{2} \sum_{i=1}^N \left(t_i - (\bm{X}\bm{w})_i\right)^2 \\
    &=&
    \frac{N}{2}\ln{\beta} - \frac{N}{2}\ln{2\pi} - \frac{\beta}{2} \norm{\bm{t} - \bm{Xw}}^2 
\end{eqnarray}

The posterior probability to determine the parameters is given as the product of likelihood function and prior.
\begin{equation} 
    p(\bm{w} \given \bm{t}, \bm{X}, \beta) \propto p(\bm{t} \given \bm{X}, \bm{w}, \beta) p(\bm{w})
\end{equation}
The value of the prior $p(\bm{w}) = 1$. 

By maximizing the negative likelihood (or posterior distribution with prior as one) with respect to $\bm{w}$,
\begin{eqnarray}
    \frac{\partial }{\partial  \bm{w}} \left\{ -\ln{p(\bm{w} \given \bm{t}, \bm{X}, \beta)} \right\} 
    &\overset{!}{=} 0& \\
    \frac{\partial }{\partial  \bm{w}} \left\{ -\ln{p(\bm{t} \given \bm{X}, \bm{w}, \beta)} \right\} 
    &\overset{!}{=} 0& \\
    \frac{\partial }{\partial \bm{w}} \left\{\frac{\beta}{2} \norm{\bm{t} - \bm{Xw}}^2  \right\}
    &\overset{!}{=} 0& \\
    \frac{\partial }{\partial \bm{w}} \left\{\frac{\beta}{2} \left(\bm{t} - \bm{X}\bm{w}\right)^T \left(\bm{t} - \bm{X}\bm{w}\right) \right\}
    &\overset{!}{=} 0& \\
    \beta \left( \bm{X}^T\bm{X}\bm{w} - \bm{X}^T \bm{t} \right) 
    &\overset{!}{=} 0& 
\end{eqnarray}

Therefore, $\mle{\bm{w}}$ is evaluated.
\begin{equation} \label{eqn:w_ML}
    \mle{\bm{w}} = (\bm{X}^T \bm{X})^{-1} \bm{X}^T \bm{t}
\end{equation}

It can be seen that the maximum likelihood results into least square estimator. We can similarly estimate $\mle{\beta}$ by maximizing the posterior with respect to $\beta$. The known value of $\mle{\bm{w}}$ can now be utilized here.

Taking the log likelihood in equation \ref{eqn:log_likelihood}, the following could be shown.
\begin{eqnarray}
    \frac{\partial }{\partial  \beta} \left\{ -\ln{p(\mle{\bm{w}} \given \bm{t}, \bm{X}, \beta)} \right\} 
    &\overset{!}{=} 0& \\
    \frac{\partial }{\partial  \beta} \left\{ -\ln{p(\bm{t} \given \bm{X}, \mle{\bm{w}}, \beta)} \right\} 
    &\overset{!}{=} 0& \\
    \frac{\partial }{\partial \beta} \left\{-\frac{N}{2}\ln{\beta} + \frac{\beta}{2} \norm{\bm{t} - \bm{X}\mle{\bm{w}}}^2  \right\}
    &\overset{!}{=} 0& \\
    \norm{\bm{t} - \bm{X} \mle{\bm{w}}}^2 - \frac{N}{\beta}
    &\overset{!}{=} 0& 
\end{eqnarray}

Therefore, the value $\mle{\beta}$ is determined to be 
\begin{equation} \label{eqn:beta_ML}
    \mle{\beta} = \frac{N}{\norm{\bm{t} - \bm{X}\mle{\bm{w}}}^2}
\end{equation}

\section{Maximum a posteriori estimation}

In the case of maximum a posteriori (MAP) estimation, the distribution of prior over parameters is known. 

\subsection{Gaussian distribution of prior}

The prior distribution is given as follows
\begin{equation} \label{prior_alpha}
    p(\bm{w} \given \alpha) = \mathcal N(\bm 0, \eye /\alpha) = \left( \frac{\alpha}{2\pi}\right)^{M/2} \exp{\left\{ -\frac{\alpha}{2}\norm{\bm{w}}^2\right\}}
\end{equation}

The posterior distribution is shown as follows.
\begin{equation}
    p(\bm{w} \given \bm{t}, \bm{X}, \alpha, \beta) \propto p(\bm{t} \given \bm{X}, \bm{w}, \beta) p(\bm{w} \given \alpha)
\end{equation}

where $\beta$ as defined earlier is the precision parameter of the likelihood, $\alpha$ is the hyperparameter (also a precision parameter of the prior distribution) which controls the distribution of model parameters. It is assumed that the value of $\alpha$ and $\beta$ is known.

The log of the posterior is given as follows
\begin{equation}
    \ln{p(\bm{w} \given \bm{t}, \bm{X}, \alpha, \beta)} = 
    \ln{p(\bm{t} \given \bm{X}, \bm{w}, \beta)} + \ln{p(\bm{w} \given \alpha)}
\end{equation}

The log likelihood is known from equation \ref{eqn:log_likelihood}. Therefore, 
\begin{equation} \label{eqn:log_posterior}
    \ln{p(\bm{w} \given \bm{t}, \bm{X}, \alpha, \beta)} = \frac{N}{2}\ln{\beta} - \frac{N}{2}\ln{2\pi} - \frac{\beta}{2} \norm{\bm{t} - \bm{Xw}}^2 + \frac{M}{2} \ln{\left( \frac{\alpha}{2\pi} \right)} - \frac{\alpha}{2}\norm{\bm{w}}^2
\end{equation}


Maximizing the negative log of posterior with respect to $\bm{w}$.
\begin{eqnarray}
    \frac{\partial }{\partial  \bm{w}} \left\{ -\ln{p(\bm{w} \given \bm{t}, \bm{X}, \alpha, \beta)} \right\} 
    &\overset{!}{=} 0& \\
    \frac{\partial }{\partial \bm{w}} \left\{\frac{\beta}{2} \left[ \left(\bm{t} - \bm{X}\bm{w}\right)^T \left(\bm{t} - \bm{X}\bm{w}\right)\right] + \frac{\alpha}{2}\bm{w}^T\bm{w}\right\}
    &\overset{!}{=} 0& \\
    \beta \left( \bm{X}^T\bm{X}\bm{w} - \bm{X}^T \bm{t} \right) + \alpha\bm{w}
    &\overset{!}{=} 0& 
\end{eqnarray}

Let us assign a regularization parameter $\lambda = \alpha/\beta$. Therefore, the value of parameter using maximum a posteriori estimation $\map{\bm{w}}$ is given as
\begin{equation} \label{eqn:w_map}
    \map{\bm{w}} = \left(\bm{X}^T\bm{X} + \lambda\eye\right)^{-1}\bm{X}^T\bm{t}
\end{equation}

\subsection{Jeffreys prior}
\subsubsection{Evaluation for precision parameter $\bm{\beta}$}

Jeffreys prior is given as
\begin{equation}
    p(\sigma) = \frac{1}{\sigma} 
\end{equation}

Parameter transformation: $\sigma\to\beta=1/\sigma^2$
  \begin{eqnarray*}
    p_\beta(\beta) &=& p_\sigma(\sigma)\left|_{\sigma=1/\sqrt{\beta}}\right. \left|\frac{d\sigma}{d\beta} \right| \\
    &\propto& \sqrt{\beta} \frac{1}{1/\sigma^3}\left|_{\sigma=1/\sqrt\beta}\right. \\
    &=& \sqrt{\beta} \sigma^3\left|_{\sigma=1/\sqrt\beta}\right. \\
    &=& \sqrt{\beta} \beta^{-3/2} = 1 / \beta
  \end{eqnarray*}

Therefore,
\begin{equation} \label{eqn:beta_jeffreys}
    p(\beta) = \frac{1}{\beta}
\end{equation}

The posterior distribution is given as
\begin{equation}
    p(\bm{w} \given \bm{t}, \bm{X}, \beta) \propto p(\bm{t} \given \bm{X}, \bm{w}, \beta) p(\beta)
\end{equation}

The log likelihood of the posterior distribution is given as
\begin{equation} 
    \ln{p(\bm{w} \given \bm{t}, \bm{X}, \beta)} = \ln{p(\bm{t} \given \bm{X}, \bm{w}, \beta)} + \ln{p(\beta)}
\end{equation}

Using equation \ref{eqn:log_likelihood} to give the log likelihood, the above equation would be as follows
\begin{equation} \label{eqn:log_posterior_jeffreys}
    \ln{p(\bm{w} \given \bm{t}, \bm{X}, \beta)} = \frac{N}{2}\ln{\beta} - \frac{N}{2}\ln{2\pi} - \frac{\beta}{2} \norm{\bm{t} - \bm{Xw}}^2 - \ln{\beta}
\end{equation}

Maximizing the log posterior with respect to $\bm{w}$,
\begin{eqnarray}
    \frac{\partial }{\partial  \bm{w}} \left\{ -\ln{p(\bm{w} \given \bm{t}, \bm{X}, \beta)} \right\} 
    &\overset{!}{=} 0& \\
    \frac{\partial }{\partial \bm{w}} \left\{\frac{\beta}{2} \norm{\bm{t} - \bm{Xw}}^2  \right\}
    &\overset{!}{=} 0& \\
    \frac{\partial }{\partial \bm{w}} \left\{\frac{\beta}{2} \left(\bm{t} - \bm{X}\bm{w}\right)^T \left(\bm{t} - \bm{X}\bm{w}\right) \right\}
    &\overset{!}{=} 0& \\
    \beta \left( \bm{X}^T\bm{X}\bm{w} - \bm{X}^T \bm{t} \right) 
    &\overset{!}{=} 0& 
\end{eqnarray}

Therefore, we find that, $\map{\bm{w}}$ is
\begin{equation}
    \map{\bm{w}} = (\bm{X}^T \bm{X})^{-1} \bm{X}^T \bm{t}
\end{equation}

We can see that $\map{\bm{w}} = \mle{\bm w}$ (refer equation \ref{eqn:w_ML}). 

Now, maximizing the log posterior with respect to $\beta$, equation \ref{eqn:log_posterior_jeffreys} is used.
\begin{eqnarray}
    \frac{\partial }{\partial  \beta} \left\{ -\ln{p(\map{\bm{w}} \given \bm{t}, \bm{X}, \beta)} \right\} 
    &\overset{!}{=} 0& \\
    \frac{\partial }{\partial \beta} \left\{\frac{\beta}{2} \norm{\bm{t} - \bm{X}\map{\bm{w}}}^2 \right\}
    - \frac{\partial }{\partial \beta} \left\{\frac{N}{2}\ln{\beta} \right\} + \frac{\partial }{\partial \beta} \left\{ \ln{\beta} \right\}
    &\overset{!}{=} 0& \\
    \frac{1}{2} \norm{\bm{t} - \bm{X}\map{\bm{w}}}^2 - \frac{N}{2\beta} + \frac{1}{\beta}
    &\overset{!}{=} 0&
\end{eqnarray}

Therefore, 
\begin{equation} \label{beta_map}
    \map{\beta} = \frac{N-2}{\norm{\bm{t} - \bm{X}\map{\bm{w}}}^2}
\end{equation}

\subsubsection{Evaluation for hyperparameter $\bm{\alpha}$}

Jeffreys prior for the hyperparameter $\alpha$ is given as follows,
\begin{equation} \label{eqn:alpha_jeffreys}
    p(\alpha) = \frac{1}{\alpha}
\end{equation}

The joint distribution $p(w,\alpha)$ is given as follows
\begin{equation} \label{eqn:joint_alpha}
    p(\bm{w}, \alpha) = p(\bm{w} \given \alpha)p(\alpha)
\end{equation}

The distribution $p(\bm{w} \given \alpha)$ is known from equation \ref{prior_alpha}. Therefore, the joint probability $p(w, \alpha)$ is evaluated as follows.
\begin{eqnarray}
    p(\bm{w}, \alpha) 
    &=&
    \left( \frac{\alpha}{2\pi}\right)^{M/2} \exp{\left\{ -\frac{\alpha}{2}\norm{\bm{w}}^2\right\}} \times \frac{1}{\alpha} \\
    p(\bm{w}, \alpha) 
    &=& 
    \left( \frac{\alpha}{2\pi}\right)^{\left( M-2\right)/2} \exp{\left\{ -\frac{\alpha}{2}\norm{\bm{w}}^2\right\}}
\end{eqnarray}

The posterior distribution is given as follows.
\begin{equation}
    p(\bm{w}, \alpha, \beta \given \bm{t}, \bm{X}) \propto p(\bm{t} \given \bm{X}, \bm{w}, \alpha, \beta) p(\bm{w}, \alpha) p(\beta)
\end{equation}

The distribution of $p(\beta)$ is known from equation \ref{eqn:beta_jeffreys} to be $p(\beta) = 1/\beta$. Therefore, the log of the posterior distribution is
\begin{equation}
    \ln{p(\bm{w}, \alpha, \beta \given \bm{t}, \bm{X})} = \ln{p(\bm{t} \given \bm{X}, \bm{w}, \alpha, \beta)} + \ln{p(\bm{w}, \alpha)} + \ln{p(\beta)}
\end{equation}

Evaluating the above equation results to the following.
\begin{equation} \label{eqn:log_posterior_alpha}
    \ln{p(\bm{w}, \alpha, \beta \given \bm{t}, \bm{X})} = \frac{N}{2}\ln{\beta}  - \frac{\beta}{2} \norm{\bm{t} - \bm{Xw}}^2 + \frac{\left(M-2\right)}{2} \ln{\alpha} - \frac{\alpha}{2}\norm{\bm{w}}^2 - \ln{\beta}
\end{equation}

Initially, maximizing the posterior (refer equation \ref{eqn:log_posterior_alpha}) with respect to $\bm{w}$, we get
\begin{eqnarray}
    \frac{\partial }{\partial  \bm{w}} \left\{ -\ln{p(\bm{w}, \alpha, \beta \given \bm{t}, \bm{X})} \right\} 
    &\overset{!}{=} 0& \\
    \frac{\partial }{\partial \bm{w}} \left\{\frac{\beta}{2} \left[ \left(\bm{t} - \bm{X}\bm{w}\right)^T \left(\bm{t} - \bm{X}\bm{w}\right)\right] + \frac{\alpha}{2}\bm{w}^T\bm{w}\right\}
    &\overset{!}{=} 0& \\
    \beta \left( \bm{X}^T\bm{X}\bm{w} - \bm{X}^T \bm{t} \right) + \alpha\bm{w}
    &\overset{!}{=} 0& 
\end{eqnarray}

Given the regularization parameter $\lambda = \alpha/\beta$, $\map{\bm{w}}$ is the same as per the equation \ref{eqn:w_map}.
\begin{equation}
    \map{\bm{w}} = \left(\bm{X}^T\bm{X} + \lambda\eye\right)^{-1}\bm{X}^T\bm{t}
\end{equation}

Finally, maximizing posterior(refer equation \ref{eqn:log_posterior_alpha}) with respect to $\beta$, we get the following
\begin{eqnarray}
    \frac{\partial }{\partial  \beta} \left\{ -\ln{p(\map{\bm{w}}, \alpha, \beta \given \bm{t}, \bm{X})} \right\} 
    &\overset{!}{=} 0& \\
    \frac{\partial }{\partial \beta} \left\{\frac{\beta}{2} \norm{\bm{t} - \bm{X}\map{\bm{w}}}^2 \right\}
    - \frac{\partial }{\partial \beta} \left\{\frac{N}{2}\ln{\beta} \right\} + \frac{\partial }{\partial \beta} \left\{ \ln{\beta} \right\}
    &\overset{!}{=} 0& \\
    \frac{1}{2} \norm{\bm{t} - \bm{X}\map{\bm{w}}}^2 - \frac{N}{2\beta} + \frac{1}{\beta}
    &\overset{!}{=} 0&
\end{eqnarray}

Therefore, $\map{\beta}$ is as follows.
\begin{equation}
    \map{\beta} = \frac{N-2}{\norm{\bm{t} - \bm{X}\map{\bm{w}}}^2}
\end{equation}

It can also be seen that $\map{\beta} = \mle{\beta}$ (refer equation \ref{eqn:beta_ML}).

Maximizing the posterior (refer equation \ref{eqn:log_posterior_alpha}) with respect to $\alpha$, we get the following.
\begin{eqnarray}
    \frac{\partial }{\partial  \alpha} \left\{ -\ln{p(\map{\bm{w}}, \alpha, \map{\beta} \given \bm{t}, \bm{X})} \right\}
    &\overset{!}{=} 0&  \\
    \norm{\map{\bm{w}}}^2 - \frac{M-2}{\alpha}
    &\overset{!}{=} 0& 
\end{eqnarray}

Therefore, $\map{\alpha}$ is given as follows.
\begin{equation}\label{alpha_map}
    \map{\alpha} = \frac{M-2}{\norm{\map{\bm{w}}}^2}
\end{equation}

\section{Gamma Distribution}

The likelihood function for the target values given $\beta$ (precision parameter) can be written as 
\begin{eqnarray}
    p(\bm{t} \given \bm{X}, \bm{w}, \beta) &=& \prod_{i=1}^N \mathcal{N}(t_i \given \bm{X}, \bm{w}, \beta^{-1}) \\
    &\propto& \beta^{\frac{N}{2}}\exp\left\{-\frac{\beta}{2} \sum_{i=1}^N(t_i - (\bm{X}\bm{w})_i)^2 \right\}
\end{eqnarray}

The corresponding conjugate prior\footnote{posterior distribution is in the same probability distribution family as the prior distribution} should be proportional to the product of the power of $\beta$ and the exponential of the linear function of $\beta$. 

Gamma distribution is a conjugate prior to several likelihood distributions (Gaussian, Poisson, exponential, etc). For example, gamma distribution over precision parameter $\beta$ can be given as follows.
\begin{equation} \label{gamma_dist}
    \text{Gam}\left(\beta \given a, b\right) = \frac{b^a}{\Gamma(a)}\beta^{a-1}\exp{(-b\beta)}
\end{equation}

where, $\Gamma(a)$ is a gamma function that ensures the gamma distribution is properly normalized, $a$ is called as the shape parameter and $b$ is called as the rate parameter.  

The expectation $\mean{\beta}$ is given as follows.
\begin{equation}
    \mean{\beta} = \frac{a}{b}
\end{equation}

The mode, $\text{mode}\left[\beta\right]$ which is equivalent to maximizing the posterior with respect to $\beta$ is given as follows.
\begin{equation} \label{mode}
    \text{mode}\left[\beta\right] = \frac{a-1}{b}
\end{equation}

Let us consider the posterior distribution considering the Jeffreys prior for $p(\beta)$ as given in equation \ref{eqn:beta_jeffreys}.
\begin{eqnarray}
    p(\bm{w} \given \bm{t}, \bm{X}, \beta) 
    &\propto& p(\bm{t} \given \bm{X}, \bm{w}, \beta) p(\beta) \\
    &\propto& \beta^{\frac{N}{2}}\exp\left\{-\frac{\beta}{2} \sum_{i=1}^N(t_i - (\bm{X}\bm{w})_i)^2 \right\} \times \frac{1}{\beta} \\
    &\propto& \beta^{\frac{N}{2}-1}\exp\left\{-\frac{\beta}{2} \sum_{i=1}^N(t_i - (\bm{X}\bm{w})_i)^2 \right\}
\end{eqnarray}

By comparing this to the gamma distribution in equation \ref{gamma_dist}, $\text{mode}\left[\beta \given \bm{t}, \bm{X}, \bm{w}\right]$ is as follows
\begin{equation}
    \text{mode}\left[\beta \given \bm{t}, \bm{X}, \bm{w}\right] = \frac{N-2}{\norm{\bm{t} - \bm{X}\bm{w}}^2}
\end{equation}

This is equivalent to $\map{\beta}$ from the result in equation \ref{beta_map}. Similarly, $\mean{\beta \given \bm{t}, \bm{X}, \bm{w}}$ is given as follows.
\begin{equation}
    \mean{\beta \given \bm{t}, \bm{X}, \bm{w}} = \frac{N}{\norm{\bm{t} - \bm{X}\bm{w}}^2}
\end{equation}

This can also be shown to hold true for the joint distribution $p(\bm{w}, \alpha)$ in equation \ref{eqn:joint_alpha}.
\begin{equation}
    p(\bm{w},\alpha) \propto \alpha^{\frac{M}{2}-1}\exp{\left\{ -\frac{\alpha}{2}\norm{\bm{w}}^2\right\}}
\end{equation}
\begin{equation}
    \text{mode}\left[\alpha \given \bm{w}\right] = \frac{M-2}{\norm{\map{\bm{w}}}^2}
\end{equation}

This is same as $\alpha_{MAP}$ in equation \ref{alpha_map}. Similarly $\mean{\alpha \given \bm{w}}$ is
\begin{equation}
    \mean{\alpha \given \bm{w}} = \frac{M}{\norm{\map{\bm{w}}}^2}
\end{equation}

\subsection{Gamma Priors with Jeffreys Priors}

Jeffreys priors are given as $p(\alpha) = \alpha^{-1}$ and $p(\beta) = \beta^{-1}$ for hyperparameter $\alpha$ (refer equation \ref{eqn:alpha_jeffreys}) and precision parameter $\beta$ (refer equation \ref{eqn:beta_jeffreys}) respectively. 

From the gamma distribution in equation \ref{gamma_dist}, a prior distribution $p(\alpha \given a_{\alpha}, b_{\alpha})$ can be given as follows.
\begin{equation} \label{gamma_alpha}
    p(\alpha \given a_{\alpha}, b_{\alpha}) =  \frac{b_{\alpha}^{a_{\alpha}}}{\Gamma(a_{\alpha})}\alpha^{a_{\alpha}-1}\exp{(-b_{\alpha}\alpha)}
\end{equation}

Similarly for $p(\beta \given a_{\beta}, b_{\beta})$ can be given as follows.
\begin{equation}\label{gamma_beta}
    p(\beta \given a_{\beta}, b_{\beta}) =  \frac{b_{\beta}^{a_{\beta}}}{\Gamma(a_{\beta})}\beta^{a_{\beta}-1}\exp{(-b_{\beta}\beta)}
\end{equation}

In the special case when $a_{\alpha}=0, b_{\alpha}=0, a_{\beta} = 0, b_{\beta}=0$. The gamma distribution above results to Jeffreys priors $p(\alpha)$ and $p(\beta)$. 

The posterior distribution $p(\alpha \given a_{\alpha}, b_{\alpha}, \bm{w})$ is therefore,
\begin{eqnarray} \label{eqn:posterior_gamma_alpha}
    p(\alpha \given a_{\alpha}, b_{\alpha}, \bm{w}) 
    &=& p(\bm{w} \given \alpha)p(\alpha \given a_{\alpha}, b_{\alpha})\\
    &=& \left( \frac{\alpha}{2\pi}\right)^{M/2} \exp{\left\{ -\frac{\alpha}{2}\norm{\bm{w}}^2\right\}}\frac{b_{\alpha}^{a_{\alpha}}}{\Gamma(a_{\alpha})}\alpha^{a_{\alpha}-1}\exp{(-b_{\alpha}\alpha)}
\end{eqnarray}

Similarly, the posterior distribution $p(\beta \given a_{\beta}, b_{\beta}, \bm{w})$
\begin{eqnarray}\label{eqn:posterior_gamma_beta}
    p(\beta \given a_{\beta}, b_{\beta}, \bm{w}) 
    &=& p(\bm{w} \given \beta)p(\beta \given a_{\beta}, b_{\beta})\\
    &=& \left( \frac{\beta}{2\pi}\right)^{N/2}\exp\left\{-\frac{\beta}{2}\norm{\bm{t}-\bm{X}\bm{w}}^2 \right\}\frac{b_{\beta}^{a_{\beta}}}{\Gamma(a_{\beta})}\beta^{a_{\beta}-1}\exp{(-b_{\beta}\beta)}
\end{eqnarray}

Initially, maximizing log posterior from equation \ref{eqn:posterior_gamma_alpha} with respect to alpha.
\begin{eqnarray}
    \frac{\partial }{\partial  \alpha} \left\{ -\ln{p(\alpha \given a_{\alpha}, b_{\alpha}, \bm{w})} \right\}
    &\overset{!}{=} 0&  \\
    \frac{\alpha}{2}\norm{\bm{w}}^2 - \frac{M}{2}\ln{\alpha} - \left( a_{\alpha} - 1 \right)\ln{\alpha} + b_{\alpha}\alpha
    &\overset{!}{=} 0&
\end{eqnarray}

Therefore, $\alpha_{MAP}$ is given as 
\begin{equation} \label{eqn:alpha_map_gamma}
    \alpha_{MAP} = \frac{M-2 + 2a_{\alpha}}{\norm{\bm{w}}^2+2b_{\alpha}}
\end{equation}

In the special case, when $a_{\alpha} = 0$ and $b_{\alpha} = 0$, the above equation is equal to \ref{alpha_map}.

Similarly, maximizing log posterior from equation \ref{eqn:posterior_gamma_beta} with respect to beta,
\begin{eqnarray}
    \frac{\partial }{\partial  \beta} \left\{ -\ln{p(\beta \given a_{\beta}, b_{\beta}, \bm{w})} \right\}
    &\overset{!}{=} 0&  \\
    \frac{\beta}{2}\norm{\bm{t}-\bm{X}\bm{w}}^2 - \frac{N}{2}\ln{\beta} - \left( a_{\beta} - 1 \right)\ln{\beta} + b_{\beta}\beta
    &\overset{!}{=} 0&
\end{eqnarray}

Therefore, $\beta_{MAP}$ is given as follows
\begin{equation}
    \beta_{MAP} = \frac{N-2 + 2a_{\beta}}{\norm{\bm{t}-\bm{X}\bm{w}}^2+2b_{\beta}}
\end{equation}
In the special case, $a_{\beta} = 0$ and $b_{\beta} = 0$, the above equation is equal to \ref{beta_map}.

Under the assumption that $a_{\alpha} = b_{\alpha} = a_{\beta} = b_{\beta} = \epsilon$, expectation, variance and mode can be given as follows.

\begin{eqnarray}
    \mean{\alpha}
    &=& \frac{a_{\alpha}}{b_{\alpha}} = 1
\end{eqnarray}
\begin{eqnarray}
    \text{var} \left[\alpha\right]
    &=& \frac{a_{\alpha}}{b_{\alpha}^2} = \frac{1}{\epsilon}
\end{eqnarray}
\begin{eqnarray}
    \text{mode}\left[\alpha\right] &=& \frac{a_{\alpha}-1}{b_{\alpha}} = \frac{\epsilon-1}{\epsilon}
\end{eqnarray}

The results are similar to $\mean{\beta}$, $\text{var}\left[\beta\right]$ and $\text{mode}\left[\beta\right]$.

If the value of $\sigma$ is known, $a_{\beta}$ and $b_{\beta}$ can be estimated.
\begin{eqnarray}
    \mean{\beta} &=& \frac{a_{\beta}}{b_{\beta}} = \frac{1}{\sigma^2}
\end{eqnarray}

The $\text{var}\left[\beta\right]$ is also known. Let us assume $\text{var}\left[\beta\right] = \varepsilon$.
\begin{eqnarray}
    \text{var}\left[\beta\right]
    &=& \mathbb{E}\big[\beta^2\big] - \mathbb{E}\big[\beta\big]^2 \\
    &=& \frac{a_{\beta}}{b_{\beta}} = \varepsilon
\end{eqnarray}

Therefore this results to the following.
\begin{equation}
    a_{\beta} = \frac{1}{\varepsilon\sigma^4}
\end{equation}
\begin{equation}
    b_{\beta} = \frac{1}{\varepsilon\sigma^2}
\end{equation}

\section{Laplace distribution}

The Laplace distribution is also called as the double exponential distribution because it can be thought of as 2 exponential distributions spliced together back-to-back. A random variable has a Laplace($\mu, \sigma$) distribution if its probability density function is given as follows.

\begin{equation}
    f(x \given \mu, \sigma) = \frac{1}{2\sigma}\exp{-\frac{\left\lvert x - \mu \right\rvert}{\sigma}}
\end{equation}

where, $\mu$ is the location parameter and $\sigma$ as scale parameter. 

Under the assumption that every output/target values follows a Laplace distribution. The likelihood  $p(t_i \given \bm{X}, \bm{w}, \sigma)$ following the Laplace distribution is given as follows.
\begin{equation}
    p(t_i \mid \bm{X}, \bm{w}, \sigma) = \frac{1}{2\sigma}\exp\left\{{-\frac{\left\lvert t_i - (\bm{X w})_i \right\lvert}{\sigma}}\right\}
\end{equation}

Let us define a precision parameter $\beta$ which is given as follows.
\begin{equation}
    \beta = \frac{1}{\sigma}
\end{equation}
According to the product rule, the total likelihood is given as the product of the individual marginal probabilities as follows.
\begin{equation}\label{eqn:total_laplace_likelihood}
    p(\bm{t} \mid \bm{X}, \bm{w}, \beta) = \prod_{i=1}^Np(t_i \mid \bm{X}, \bm{w}, \beta)
\end{equation}
The log likelihood of the above equation \ref{eqn:total_laplace_likelihood} can be given as follows.
\begin{eqnarray}
    \ln{p(\bm{t} \mid \bm{X}, \bm{w}, \beta)} 
    &=& 
    \sum_{i=1}^N \ln p(t_i \given \bm{X}, \bm{w}, \beta) \\
    &=&
    \sum_{i=1}^N \ln{\left\{\frac{\beta}{2}\exp{\left(-\beta \left\lvert t_i - (\bm{X w})_i \right\lvert \right)}\right\}} \\
    &=&
    \frac{N}{2}\ln{\beta} - \beta \sum_{i=1}^N \left\lvert t_i - (\bm{X w})_i \right\lvert \\
    &=&
    \frac{N}{2}\ln{\beta} - \beta \norm{\bm{t} - \bm{X w}}
\end{eqnarray}

By maximizing the log likelihood with respect to $\bm{w}$, the value of $W_{ML}$ can be found. However, it cannot be treated analytically. Therefore, a suitable optimizer such as Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm could be used. 

After evaluating $\mle{\bm{w}}$, $\mle{\beta}$ could be found out by maximizing the log likelihood with respect to $\beta$. 
\begin{eqnarray}
    \frac{\partial }{\partial  \beta} \left\{ -\ln{p(\bm{t} \given \bm{X}, \mle{\bm{w}}, \beta)} \right\} 
    &\overset{!}{=} 0& \\
    \frac{\partial }{\partial  \beta} \left\{\beta \norm{\bm{t} - \bm{X w}} - \frac{N}{2}\ln{\beta} \right\}
    &\overset{!}{=} 0&
\end{eqnarray}


\end{document}